{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e23e0d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_ok\n"
     ]
    }
   ],
   "source": [
    "print(\"all_ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf594cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c43bc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79b630d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89c45b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ChatGroq(model=\"qwen/qwen3-32b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7710b9d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"<think>\\nOkay, the user is asking for the capital of France. Let me think. I remember that major countries have well-known capitals. France is a European country, right? I think the capital is Paris. Wait, is there any other possibility? Sometimes people might confuse it with Lyon or Marseille, but those are cities in France. Paris is definitely the capital. Let me confirm in my mind. Yeah, Paris is where the government is, the Eiffel Tower is there, and it's a major city. I don't think there's any recent change. So the answer should be Paris.\\n</think>\\n\\nThe capital of France is **Paris**. It is known for its cultural landmarks, such as the Eiffel Tower, the Louvre Museum, and its historical significance as a center of art, fashion, and politics.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 170, 'prompt_tokens': 15, 'total_tokens': 185, 'completion_time': 0.442754104, 'prompt_time': 0.000426744, 'queue_time': 0.064321117, 'total_time': 0.443180848}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_f17c2eb555', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--609b7608-20ba-4459-89d2-b0620875ca50-0', usage_metadata={'input_tokens': 15, 'output_tokens': 170, 'total_tokens': 185})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "135737c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd6d1e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model=GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\",\n",
    "    dimensions=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f85a34d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.04049589857459068,\n",
       " -0.04480205848813057,\n",
       " -0.03519536182284355,\n",
       " -0.039978861808776855,\n",
       " 0.060929879546165466,\n",
       " -0.0020229103974997997,\n",
       " 0.0018945873016491532,\n",
       " -0.02098630554974079,\n",
       " 0.00390483345836401,\n",
       " 0.07453471422195435,\n",
       " -0.018631644546985626,\n",
       " 0.007455417420715094,\n",
       " -0.005498720332980156,\n",
       " 0.0277552530169487,\n",
       " 0.02441469207406044,\n",
       " -0.04078073427081108,\n",
       " 0.006169943604618311,\n",
       " 0.028121715411543846,\n",
       " 0.047293782234191895,\n",
       " -0.011507853865623474,\n",
       " 0.01713664084672928,\n",
       " 0.008143678307533264,\n",
       " -0.03534378856420517,\n",
       " 0.024886861443519592,\n",
       " 0.018857652321457863,\n",
       " -0.05387217551469803,\n",
       " 0.012068687006831169,\n",
       " -0.04790126904845238,\n",
       " -0.012439027428627014,\n",
       " -0.0018709597643464804,\n",
       " -0.07322046160697937,\n",
       " 0.04119649529457092,\n",
       " 0.003926463890820742,\n",
       " -0.0114192645996809,\n",
       " 0.027035439386963844,\n",
       " -0.05288225784897804,\n",
       " -0.004346303641796112,\n",
       " 0.023509696125984192,\n",
       " -0.00741075212135911,\n",
       " 0.0356050580739975,\n",
       " -0.00813733134418726,\n",
       " -0.009293914772570133,\n",
       " -0.04593217745423317,\n",
       " 0.01098004449158907,\n",
       " -0.010773461312055588,\n",
       " -0.01597120612859726,\n",
       " -0.024269158020615578,\n",
       " 0.06505946815013885,\n",
       " 0.011711898259818554,\n",
       " -0.00990017969161272,\n",
       " 0.01595977507531643,\n",
       " -0.008206325583159924,\n",
       " 0.05414727330207825,\n",
       " 0.015241783112287521,\n",
       " 0.01565716415643692,\n",
       " -0.06717009097337723,\n",
       " 0.013598041608929634,\n",
       " -0.01206971611827612,\n",
       " -0.006766031496226788,\n",
       " 0.02258889749646187,\n",
       " 0.03195677697658539,\n",
       " -0.02795996703207493,\n",
       " 0.004754781257361174,\n",
       " 0.03880754113197327,\n",
       " 0.018016405403614044,\n",
       " -0.056480422616004944,\n",
       " 0.033468831330537796,\n",
       " 0.013152416795492172,\n",
       " 0.08275853842496872,\n",
       " 0.003966258838772774,\n",
       " -0.006652957759797573,\n",
       " -0.04722088947892189,\n",
       " 0.07506217807531357,\n",
       " 0.00833130069077015,\n",
       " 0.024480273947119713,\n",
       " -0.09435391426086426,\n",
       " -0.02432459406554699,\n",
       " 0.06469137221574783,\n",
       " 0.015098823234438896,\n",
       " -0.041668083518743515,\n",
       " -0.01132252812385559,\n",
       " -0.05375457555055618,\n",
       " -0.0857798382639885,\n",
       " -0.031574495136737823,\n",
       " -0.059871990233659744,\n",
       " 0.010076175443828106,\n",
       " -0.05363050475716591,\n",
       " -0.016513127833604813,\n",
       " -0.05007170885801315,\n",
       " 0.05129191651940346,\n",
       " -0.02370879054069519,\n",
       " 0.010461362078785896,\n",
       " 0.07118172198534012,\n",
       " -0.04078911244869232,\n",
       " -0.02913387306034565,\n",
       " 0.06759403645992279,\n",
       " -0.014845332130789757,\n",
       " -0.012595937587320805,\n",
       " 0.07580124586820602,\n",
       " -0.008960776962339878,\n",
       " 0.019636666402220726,\n",
       " -0.02066907100379467,\n",
       " -0.06830579042434692,\n",
       " 0.04878520220518112,\n",
       " 0.03572282940149307,\n",
       " -0.012236935086548328,\n",
       " -0.010849707759916782,\n",
       " 0.05455821380019188,\n",
       " 0.007740095257759094,\n",
       " 0.0630125179886818,\n",
       " -0.07112842798233032,\n",
       " -0.045658886432647705,\n",
       " 0.028484303504228592,\n",
       " 0.05648983642458916,\n",
       " 0.01675332337617874,\n",
       " -0.052414052188396454,\n",
       " -0.028778357431292534,\n",
       " 0.021177630871534348,\n",
       " 0.040066733956336975,\n",
       " 0.02611405774950981,\n",
       " 0.03870968148112297,\n",
       " -0.01853475719690323,\n",
       " 0.04437531530857086,\n",
       " -0.04438794031739235,\n",
       " 0.03159865736961365,\n",
       " -0.016245849430561066,\n",
       " -0.048600293695926666,\n",
       " 0.03397243097424507,\n",
       " -0.0050695547834038734,\n",
       " 0.015177370980381966,\n",
       " 0.00915257353335619,\n",
       " -0.047043170779943466,\n",
       " -0.02225576899945736,\n",
       " -0.0011699205497279763,\n",
       " 0.01987353153526783,\n",
       " 0.03717658296227455,\n",
       " 0.05481540039181709,\n",
       " -0.014918218366801739,\n",
       " 0.04227130860090256,\n",
       " 0.004491603001952171,\n",
       " 0.02655336633324623,\n",
       " 0.06508819013834,\n",
       " 0.0050729988142848015,\n",
       " 0.029402025043964386,\n",
       " -0.019047273322939873,\n",
       " 0.05203188210725784,\n",
       " -0.061927396804094315,\n",
       " -0.03418806567788124,\n",
       " 0.043734438717365265,\n",
       " -0.04346655681729317,\n",
       " -0.07404075562953949,\n",
       " 0.002955378033220768,\n",
       " -0.04757191985845566,\n",
       " -0.026957297697663307,\n",
       " 0.04407310113310814,\n",
       " 0.0046082898043096066,\n",
       " -0.011509543284773827,\n",
       " 0.04934639111161232,\n",
       " 0.02039908431470394,\n",
       " -0.016596553847193718,\n",
       " 0.058120984584093094,\n",
       " 0.01857122592628002,\n",
       " 0.016607575118541718,\n",
       " 0.006390347145497799,\n",
       " -0.0024492843076586723,\n",
       " -0.022973939776420593,\n",
       " 0.03097490966320038,\n",
       " -0.011230415664613247,\n",
       " 0.018612513318657875,\n",
       " -0.0011642066529020667,\n",
       " -0.013932803645730019,\n",
       " 0.017299074679613113,\n",
       " -0.005183328874409199,\n",
       " -0.02756204642355442,\n",
       " 0.007250694558024406,\n",
       " -0.03435893356800079,\n",
       " 0.01172571536153555,\n",
       " -0.023683499544858932,\n",
       " -0.015510759316384792,\n",
       " -0.031465303152799606,\n",
       " 0.007509587332606316,\n",
       " -0.04015637934207916,\n",
       " 0.013057157397270203,\n",
       " 0.04102318733930588,\n",
       " 0.012030666694045067,\n",
       " -0.04409577324986458,\n",
       " 0.02795119769871235,\n",
       " -0.031370554119348526,\n",
       " 0.0020824801176786423,\n",
       " 0.022251205518841743,\n",
       " -0.04768866300582886,\n",
       " -0.011363934725522995,\n",
       " -0.022053701803088188,\n",
       " -0.02086089365184307,\n",
       " -0.03565332666039467,\n",
       " -0.010937473736703396,\n",
       " 0.029957393184304237,\n",
       " -0.014769344590604305,\n",
       " 0.043145839124917984,\n",
       " -0.05224718898534775,\n",
       " -0.024222128093242645,\n",
       " 0.038145340979099274,\n",
       " 0.018341824412345886,\n",
       " -0.028980541974306107,\n",
       " 0.01722531020641327,\n",
       " -0.0041764164343476295,\n",
       " 0.09076888859272003,\n",
       " -0.03722015395760536,\n",
       " -0.05027609318494797,\n",
       " 0.05215790867805481,\n",
       " -0.012998655438423157,\n",
       " 0.008780761621892452,\n",
       " -0.025272613391280174,\n",
       " 0.011743598617613316,\n",
       " 0.04203746095299721,\n",
       " -0.026697078719735146,\n",
       " 0.07385823875665665,\n",
       " 0.010556662455201149,\n",
       " 0.04205450043082237,\n",
       " -0.024556642398238182,\n",
       " -0.04050295427441597,\n",
       " -0.004069443326443434,\n",
       " -0.021989906206727028,\n",
       " -0.010687476955354214,\n",
       " 0.041327763348817825,\n",
       " 0.028045659884810448,\n",
       " -0.00919997040182352,\n",
       " -0.03327375277876854,\n",
       " 0.000798177148681134,\n",
       " -0.013354405760765076,\n",
       " -0.048952922224998474,\n",
       " 0.07362556457519531,\n",
       " 0.029734747484326363,\n",
       " -0.02204304374754429,\n",
       " 0.06452139467000961,\n",
       " -0.0018220851197838783,\n",
       " -0.002310924930498004,\n",
       " 0.02215779386460781,\n",
       " 0.002388277556747198,\n",
       " -0.002533142687752843,\n",
       " -0.02539815939962864,\n",
       " -0.03312728926539421,\n",
       " 0.04245744273066521,\n",
       " 0.024170683696866035,\n",
       " -0.04072323441505432,\n",
       " -0.03723141551017761,\n",
       " -0.027613334357738495,\n",
       " 0.02394406497478485,\n",
       " 0.04481898993253708,\n",
       " 0.04535920172929764,\n",
       " -0.0032587379682809114,\n",
       " -0.0004388995876070112,\n",
       " 0.03285910189151764,\n",
       " 0.01760149747133255,\n",
       " -0.08256322145462036,\n",
       " 0.027649592608213425,\n",
       " -0.06751957535743713,\n",
       " 0.030853813514113426,\n",
       " -0.03653700277209282,\n",
       " 0.03154642507433891,\n",
       " 0.03589324280619621,\n",
       " 0.03334618732333183,\n",
       " 0.038730207830667496,\n",
       " -0.03706396743655205,\n",
       " -0.011471301317214966,\n",
       " -0.03637073561549187,\n",
       " 0.0023318633902817965,\n",
       " -0.05364224314689636,\n",
       " 0.010904532857239246,\n",
       " 0.006715408526360989,\n",
       " 0.03490792214870453,\n",
       " -0.07031109184026718,\n",
       " 0.016439497470855713,\n",
       " 0.036860622465610504,\n",
       " 0.03032566048204899,\n",
       " 0.010677222162485123,\n",
       " -0.05305841565132141,\n",
       " 0.06286440789699554,\n",
       " 0.06270558387041092,\n",
       " -0.07088932394981384,\n",
       " 0.04064720869064331,\n",
       " 0.029234955087304115,\n",
       " -0.0059632412157952785,\n",
       " -0.0190551970154047,\n",
       " -0.0054027121514081955,\n",
       " -0.015841184183955193,\n",
       " -0.04716286435723305,\n",
       " -0.011090202257037163,\n",
       " 0.009548588655889034,\n",
       " -0.06007857620716095,\n",
       " -0.03337058797478676,\n",
       " -0.0807088166475296,\n",
       " 0.032452166080474854,\n",
       " -0.04080561548471451,\n",
       " -0.09995683282613754,\n",
       " 0.000732609536498785,\n",
       " -0.02875148132443428,\n",
       " 0.038151271641254425,\n",
       " 0.03327471390366554,\n",
       " -0.02466670051217079,\n",
       " -0.03210589289665222,\n",
       " 0.004603350069373846,\n",
       " 0.03380708768963814,\n",
       " -0.06126943230628967,\n",
       " 0.030772460624575615,\n",
       " 0.02966260351240635,\n",
       " -0.03604308143258095,\n",
       " -0.06102821230888367,\n",
       " 0.029228761792182922,\n",
       " 0.02995629981160164,\n",
       " 0.04501528665423393,\n",
       " -0.012613361701369286,\n",
       " -0.058307912200689316,\n",
       " -0.03434529900550842,\n",
       " -0.013430471532046795,\n",
       " 0.06727061420679092,\n",
       " -0.01931852474808693,\n",
       " -0.022273380309343338,\n",
       " 0.0172544177621603,\n",
       " 0.028084540739655495,\n",
       " 0.011966834776103497,\n",
       " 0.08038466423749924,\n",
       " 0.02990385890007019,\n",
       " -0.01733776554465294,\n",
       " -0.00425852881744504,\n",
       " 0.005884779617190361,\n",
       " 0.01429015677422285,\n",
       " 0.018236370757222176,\n",
       " 0.015414105728268623,\n",
       " -0.011564589105546474,\n",
       " -0.027001330628991127,\n",
       " 0.03680700808763504,\n",
       " -0.04364212974905968,\n",
       " 0.005125280469655991,\n",
       " 0.010493139736354351,\n",
       " 0.06202792003750801,\n",
       " -0.040758583694696426,\n",
       " 0.02475064806640148,\n",
       " -0.05432722344994545,\n",
       " 0.0058959778398275375,\n",
       " 0.04371344670653343,\n",
       " 0.049989670515060425,\n",
       " -0.019738079980015755,\n",
       " -0.018227912485599518,\n",
       " -0.017926182597875595,\n",
       " -0.017255669459700584,\n",
       " -0.015845123678445816,\n",
       " 0.019173292443156242,\n",
       " 0.09459353983402252,\n",
       " 0.02536124177277088,\n",
       " 0.001187007874250412,\n",
       " 0.08021384477615356,\n",
       " 0.00924182590097189,\n",
       " 0.04623769223690033,\n",
       " -0.0004556997155304998,\n",
       " -0.02065275050699711,\n",
       " 0.05971353501081467,\n",
       " -0.012521890923380852,\n",
       " 0.012784424237906933,\n",
       " -0.05809435993432999,\n",
       " 0.016627559438347816,\n",
       " 0.0310691986232996,\n",
       " -0.03631414473056793,\n",
       " -0.0492539219558239,\n",
       " -0.019370965659618378,\n",
       " -0.020427018404006958,\n",
       " -0.010020453482866287,\n",
       " 0.002172649372369051,\n",
       " 0.018464308232069016,\n",
       " 0.022155364975333214,\n",
       " 0.015383323654532433,\n",
       " 0.010965285822749138,\n",
       " 0.04771880805492401,\n",
       " -0.04354555159807205,\n",
       " 0.056955087929964066,\n",
       " 0.030178379267454147,\n",
       " -0.06719762831926346,\n",
       " -0.014524973928928375,\n",
       " 0.016225548461079597,\n",
       " 0.024253830313682556,\n",
       " -0.04209240898489952,\n",
       " -0.02529408596456051,\n",
       " 0.049950361251831055,\n",
       " 0.02480176091194153,\n",
       " 0.014506431296467781,\n",
       " -0.016044732183218002,\n",
       " 0.04721377417445183,\n",
       " 0.017429251223802567,\n",
       " -0.025577858090400696,\n",
       " 0.05111472308635712,\n",
       " -0.06414662301540375,\n",
       " 0.07264406234025955,\n",
       " 0.0637986958026886,\n",
       " -0.02453370951116085,\n",
       " -0.01815640553832054,\n",
       " -0.02826329879462719,\n",
       " -0.0064158970490098,\n",
       " -0.028589699417352676,\n",
       " 0.02806905098259449,\n",
       " 0.03566247224807739,\n",
       " -0.021823687478899956,\n",
       " -0.07231482863426208,\n",
       " -0.03792164847254753,\n",
       " -0.006366659887135029,\n",
       " -0.013961256481707096,\n",
       " 0.009016234427690506,\n",
       " 0.0014482670230790973,\n",
       " -0.007783473934978247,\n",
       " -0.05828552320599556,\n",
       " 0.01589290425181389,\n",
       " 0.011120520532131195,\n",
       " -0.01777086965739727,\n",
       " 0.021873313933610916,\n",
       " -0.04373500868678093,\n",
       " -0.01964663341641426,\n",
       " -0.009540091268718243,\n",
       " 0.023465441539883614,\n",
       " -0.016643701121211052,\n",
       " -2.818886969180312e-05,\n",
       " 0.06556988507509232,\n",
       " -0.02015743963420391,\n",
       " -0.006127714645117521,\n",
       " 0.004732717759907246,\n",
       " -0.015490321442484856,\n",
       " -0.054713305085897446,\n",
       " -0.06902629882097244,\n",
       " 0.003236546413972974,\n",
       " 0.03797807916998863,\n",
       " 0.03530682623386383,\n",
       " -0.007956664077937603,\n",
       " 0.052497345954179764,\n",
       " 0.021104488521814346,\n",
       " -0.04636838287115097,\n",
       " -0.07017705589532852,\n",
       " -0.021134676411747932,\n",
       " -0.027609067037701607,\n",
       " 0.003762602573260665,\n",
       " 0.023895254358649254,\n",
       " -0.022583821788430214,\n",
       " -0.0057598622515797615,\n",
       " 0.009686781093478203,\n",
       " -0.04235462471842766,\n",
       " 0.03800799697637558,\n",
       " 0.021413663402199745,\n",
       " -0.0774780660867691,\n",
       " 0.0004925738321617246,\n",
       " -0.009664773009717464,\n",
       " -0.017831752076745033,\n",
       " 0.004776636138558388,\n",
       " -0.06284651160240173,\n",
       " 0.02461630292236805,\n",
       " -0.026436761021614075,\n",
       " -0.0016810768283903599,\n",
       " -0.04477179795503616,\n",
       " -0.09151613712310791,\n",
       " -0.010227843187749386,\n",
       " -0.01437322422862053,\n",
       " 0.08218616247177124,\n",
       " -0.054681967943906784,\n",
       " 0.06387192010879517,\n",
       " 0.004982621408998966,\n",
       " -0.00615812698379159,\n",
       " -0.018258098512887955,\n",
       " -0.1149948239326477,\n",
       " 0.07439476251602173,\n",
       " 0.0218903049826622,\n",
       " 0.006960784085094929,\n",
       " -0.006247085984796286,\n",
       " -0.010823355987668037,\n",
       " 0.029611270874738693,\n",
       " 0.025673655793070793,\n",
       " 0.00453235162422061,\n",
       " -0.03215603157877922,\n",
       " -0.04070455580949783,\n",
       " -0.02171097695827484,\n",
       " -0.02638229914009571,\n",
       " -0.05321156606078148,\n",
       " 0.03981984406709671,\n",
       " -0.044293515384197235,\n",
       " 0.01215767115354538,\n",
       " 0.014081534929573536,\n",
       " 0.02260136604309082,\n",
       " 0.01703871227800846,\n",
       " 0.027764135971665382,\n",
       " -0.024178093299269676,\n",
       " 0.037418004125356674,\n",
       " -0.02304765395820141,\n",
       " -0.011035383678972721,\n",
       " -0.056562088429927826,\n",
       " 0.022719377651810646,\n",
       " -0.008883198723196983,\n",
       " -0.02445538341999054,\n",
       " -0.01567012444138527,\n",
       " -0.013430855236947536,\n",
       " -0.0194851066917181,\n",
       " -0.019389895722270012,\n",
       " 0.00421789800748229,\n",
       " 0.034655649214982986,\n",
       " 0.036937110126018524,\n",
       " 0.004344061017036438,\n",
       " -0.026959843933582306,\n",
       " -0.016799381002783775,\n",
       " -0.0024602345656603575,\n",
       " -0.025223901495337486,\n",
       " 0.057808876037597656,\n",
       " -0.09295155853033066,\n",
       " -0.012100448831915855,\n",
       " 0.025417771190404892,\n",
       " -0.02066008187830448,\n",
       " -0.032801464200019836,\n",
       " 0.0006481426535174251,\n",
       " 0.01030695904046297,\n",
       " 0.03199635073542595,\n",
       " 0.04073688015341759,\n",
       " 0.022221416234970093,\n",
       " -0.0029444892425090075,\n",
       " 0.01315197255462408,\n",
       " -0.004565185867249966,\n",
       " 0.018314460292458534,\n",
       " -0.029337801039218903,\n",
       " 0.01444859430193901,\n",
       " -0.025903156027197838,\n",
       " -0.09524020552635193,\n",
       " -0.002099149627611041,\n",
       " -0.029483281075954437,\n",
       " 0.019224995747208595,\n",
       " 0.022705597802996635,\n",
       " 0.053441938012838364,\n",
       " -0.0355648472905159,\n",
       " -0.0009669328574091196,\n",
       " -0.024327477440238,\n",
       " 0.020528599619865417,\n",
       " -0.010461662895977497,\n",
       " -0.02441529929637909,\n",
       " 0.03521616384387016,\n",
       " -0.0035327007062733173,\n",
       " -0.005011484492570162,\n",
       " 0.015951240435242653,\n",
       " 0.028823083266615868,\n",
       " -0.022921429947018623,\n",
       " 0.039546482264995575,\n",
       " 0.02260730229318142,\n",
       " 0.06468944996595383,\n",
       " 0.01812911033630371,\n",
       " -0.03061552345752716,\n",
       " -0.013902192935347557,\n",
       " -0.0061100199818611145,\n",
       " -0.05473892018198967,\n",
       " -0.01829078048467636,\n",
       " 0.034489043056964874,\n",
       " -0.005583097692579031,\n",
       " 0.01991846412420273,\n",
       " 0.010429474525153637,\n",
       " -0.013883198611438274,\n",
       " 0.014815482310950756,\n",
       " -0.02793223038315773,\n",
       " -0.01903199590742588,\n",
       " 0.03920678794384003,\n",
       " 0.01517440751194954,\n",
       " -0.047574203461408615,\n",
       " -0.05044238641858101,\n",
       " 0.0008338171173818409,\n",
       " -0.0022520178463310003,\n",
       " 0.024571724236011505,\n",
       " 0.08084134757518768,\n",
       " 0.025915881618857384,\n",
       " -0.016095496714115143,\n",
       " -0.015557829290628433,\n",
       " 0.06818326562643051,\n",
       " 0.0013206842122599483,\n",
       " -0.010275495238602161,\n",
       " 0.009359994903206825,\n",
       " 0.011058165691792965,\n",
       " 0.031549714505672455,\n",
       " 0.008585716597735882,\n",
       " -0.0233459435403347,\n",
       " 0.00751951988786459,\n",
       " -0.039554886519908905,\n",
       " -0.03721010312438011,\n",
       " -0.009713971987366676,\n",
       " 0.028422679752111435,\n",
       " -0.007892055436968803,\n",
       " -0.0030861650593578815,\n",
       " 0.02596166543662548,\n",
       " 0.008772616274654865,\n",
       " 0.03363153710961342,\n",
       " 0.057541683316230774,\n",
       " 0.1019878014922142,\n",
       " 0.040293339639902115,\n",
       " 0.05504341796040535,\n",
       " -0.03201066330075264,\n",
       " 0.01813836209475994,\n",
       " -0.031475428491830826,\n",
       " 0.0006648820708505809,\n",
       " 0.01786964386701584,\n",
       " 0.0076837497763335705,\n",
       " -0.018364831805229187,\n",
       " -0.028105178847908974,\n",
       " -0.013622824102640152,\n",
       " -0.015150739811360836,\n",
       " 0.05624852702021599,\n",
       " -0.02013879269361496,\n",
       " 0.0044675483368337154,\n",
       " -0.025596806779503822,\n",
       " 0.019242072477936745,\n",
       " 0.05761236697435379,\n",
       " -0.004415152594447136,\n",
       " 0.012549351900815964,\n",
       " -0.0077471681870520115,\n",
       " 0.014663511887192726,\n",
       " 0.027408527210354805,\n",
       " -0.041193991899490356,\n",
       " 0.015324910171329975,\n",
       " -0.017608411610126495,\n",
       " -0.03168262541294098,\n",
       " -0.032555632293224335,\n",
       " 0.07168321311473846,\n",
       " 0.009799330495297909,\n",
       " 0.02118111401796341,\n",
       " -0.04160045087337494,\n",
       " 0.011955291032791138,\n",
       " -0.016465919092297554,\n",
       " 0.031664226204156876,\n",
       " -0.01568051241338253,\n",
       " 0.04173530638217926,\n",
       " 0.03464770317077637,\n",
       " 0.008836270309984684,\n",
       " -0.0027669453993439674,\n",
       " 0.06807421892881393,\n",
       " 0.011814332567155361,\n",
       " 0.05693301931023598,\n",
       " 0.05131593346595764,\n",
       " -0.02981972135603428,\n",
       " 0.01084813941270113,\n",
       " -0.029661012813448906,\n",
       " -0.03380691260099411,\n",
       " -0.05593761056661606,\n",
       " -0.01662912406027317,\n",
       " 0.010600208304822445,\n",
       " -0.02008874900639057,\n",
       " -0.03735111653804779,\n",
       " -0.023617569357156754,\n",
       " 0.007238880731165409,\n",
       " -0.009406358003616333,\n",
       " 0.02834150940179825,\n",
       " 0.10475940257310867,\n",
       " 0.04639627784490585,\n",
       " -0.09643177688121796,\n",
       " -0.056376393884420395,\n",
       " -0.03042629174888134,\n",
       " -0.030333541333675385,\n",
       " -0.007821416482329369,\n",
       " 0.004392584785819054,\n",
       " -0.02059929445385933,\n",
       " 0.04922041296958923,\n",
       " 0.020816422998905182,\n",
       " -0.03358255326747894,\n",
       " -0.0491083599627018,\n",
       " 0.015258957631886005,\n",
       " 0.016441497951745987,\n",
       " -0.015753863379359245,\n",
       " -0.023902930319309235,\n",
       " 0.005782296881079674,\n",
       " 0.008558672852814198,\n",
       " 0.007462106645107269,\n",
       " 0.016703788191080093,\n",
       " -0.030533140525221825,\n",
       " -0.07188694179058075,\n",
       " -0.010673034004867077,\n",
       " 0.0475495420396328,\n",
       " -0.003992817830294371,\n",
       " 0.014632674865424633,\n",
       " 0.02534979209303856,\n",
       " 0.0018633923027664423,\n",
       " 0.042079102247953415,\n",
       " 0.024765687063336372,\n",
       " 0.0022178515791893005,\n",
       " 0.019023988395929337,\n",
       " 0.015153474174439907,\n",
       " 0.009683950804173946,\n",
       " 0.0010971790179610252,\n",
       " -0.0008446355350315571,\n",
       " -0.01728888973593712,\n",
       " 0.004079173784703016,\n",
       " -0.03167934715747833,\n",
       " 0.04870700463652611,\n",
       " 0.030338767915964127,\n",
       " 0.008246907033026218,\n",
       " -0.031554900109767914,\n",
       " -0.05660296976566315,\n",
       " 0.0036534450482577085,\n",
       " -0.014625159092247486,\n",
       " -0.06521178781986237,\n",
       " 0.053864117711782455,\n",
       " 0.0660531297326088,\n",
       " 0.019983068108558655,\n",
       " 0.03262905776500702,\n",
       " 0.007290141191333532,\n",
       " -0.0034917518496513367,\n",
       " 0.021520834416151047,\n",
       " -0.022835295647382736,\n",
       " -0.035963475704193115,\n",
       " -0.04341991990804672,\n",
       " 0.00109879020601511,\n",
       " -0.0013851616531610489,\n",
       " -0.002607055241242051,\n",
       " 0.0372546948492527,\n",
       " 0.03605737164616585,\n",
       " 0.0622076541185379,\n",
       " 0.07573757320642471,\n",
       " 0.035234738141298294,\n",
       " -0.024771325290203094,\n",
       " -0.02295812964439392,\n",
       " 0.023103782907128334,\n",
       " -0.01134227029979229,\n",
       " -0.04811793193221092,\n",
       " 0.024686969816684723,\n",
       " 0.03067523054778576,\n",
       " -0.01713031716644764,\n",
       " 0.09968256950378418,\n",
       " 0.05381736159324646,\n",
       " 0.0518648624420166,\n",
       " 0.062321122735738754,\n",
       " -0.029852405190467834,\n",
       " -0.05626152455806732,\n",
       " 0.08374395966529846,\n",
       " -0.08735101670026779,\n",
       " -0.03338709473609924,\n",
       " -0.030616890639066696,\n",
       " 0.034440141171216965,\n",
       " 0.08644913882017136,\n",
       " -0.0027444453444331884,\n",
       " -0.006311277858912945,\n",
       " -0.01766940765082836,\n",
       " 0.0023246563505381346,\n",
       " 0.08062084764242172,\n",
       " 0.03189488872885704,\n",
       " 0.0393792949616909,\n",
       " 0.0014688537921756506,\n",
       " -0.0906258225440979,\n",
       " -0.02381587214767933,\n",
       " 0.036763254553079605,\n",
       " 0.009490679018199444,\n",
       " 0.04041679576039314,\n",
       " 0.024336544796824455,\n",
       " 0.005297109019011259,\n",
       " -0.016277795657515526,\n",
       " -0.03244338184595108,\n",
       " 0.02116851508617401,\n",
       " -0.07674777507781982,\n",
       " -0.03775396570563316,\n",
       " 0.007768628187477589,\n",
       " -0.02953208051621914,\n",
       " 0.036300890147686005,\n",
       " 0.03283258527517319,\n",
       " -0.013521114364266396,\n",
       " -0.01642083004117012,\n",
       " 0.014644011855125427,\n",
       " -0.015452615916728973,\n",
       " -0.002417708281427622,\n",
       " -0.017770152539014816,\n",
       " -0.043932270258665085,\n",
       " 0.006733616814017296,\n",
       " 0.00907968357205391,\n",
       " 0.010359407402575016,\n",
       " 0.014600984752178192,\n",
       " 0.04979071021080017,\n",
       " -0.006657346151769161]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model.embed_query(\"what is the capital of France?\")  # Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fb9ce6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9712dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54e2774e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e60348d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(os.getcwd(), \"data\", \"sample.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "768eca93",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c10315cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed6565f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Llama 2: Open Foundation and Fine-Tuned Chat Models\\nHugo Touvron∗ Louis Martin† Kevin Stone†\\nPeter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\\nPrajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\\nGuillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\\nCynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\\nHakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev\\nPunit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\\nYinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra\\nIgor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\\nAlan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\\nSergey Edunov Thomas Scialom∗\\nGenAI, Meta\\nAbstract\\nIn this work, we develop and release Llama 2, a collection of pretrained and fine-tuned\\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\\nOur fine-tuned LLMs, calledLlama 2-Chat, are optimized for dialogue use cases. Our\\nmodels outperform open-source chat models on most benchmarks we tested, and based on\\nour human evaluations for helpfulness and safety, may be a suitable substitute for closed-\\nsource models. We provide a detailed description of our approach to fine-tuning and safety\\nimprovements ofLlama 2-Chatin order to enable the community to build on our work and\\ncontribute to the responsible development of LLMs.\\n∗Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com\\n†Second author\\nContributions for all the authors can be found in Section A.1.\\narXiv:2307.09288v2  [cs.CL]  19 Jul 2023'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].page_content  # Accessing the content of the first document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "140508e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)  # Number of documents loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "037db04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=150,\n",
    "    length_function=len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc2dfbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text_splitter.split_documents(documents)  # Splitting the documents into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81ad46d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "765"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3961139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'pdfTeX-1.40.25',\n",
       " 'creator': 'LaTeX with hyperref',\n",
       " 'creationdate': '2023-07-20T00:30:36+00:00',\n",
       " 'author': '',\n",
       " 'keywords': '',\n",
       " 'moddate': '2023-07-20T00:30:36+00:00',\n",
       " 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       " 'subject': '',\n",
       " 'title': '',\n",
       " 'trapped': '/False',\n",
       " 'source': 'd:\\\\llmops\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf',\n",
       " 'total_pages': 77,\n",
       " 'page': 0,\n",
       " 'page_label': '1'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata  # Accessing metadata of the first chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8027585b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Llama 2: Open Foundation and Fine-Tuned Chat Models\\nHugo Touvron∗ Louis Martin† Kevin Stone†\\nPeter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\\nPrajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\\nGuillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\\nCynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\\nHakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content  # Accessing the content of the first chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "244109b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.034186601638793945,\n",
       "  -0.026368774473667145,\n",
       "  -0.07720824331045151,\n",
       "  -0.007780628278851509,\n",
       "  0.059689391404390335,\n",
       "  0.021460626274347305,\n",
       "  0.01821916550397873,\n",
       "  -0.033960189670324326,\n",
       "  0.001744729233905673,\n",
       "  0.021980635821819305,\n",
       "  -0.011941241100430489,\n",
       "  0.023290259763598442,\n",
       "  -0.053314149379730225,\n",
       "  -0.005486520007252693,\n",
       "  0.026051264256238937,\n",
       "  -0.039452578872442245,\n",
       "  0.05533382669091225,\n",
       "  0.03397344425320625,\n",
       "  -0.02078220434486866,\n",
       "  0.025393392890691757,\n",
       "  0.004267213400453329,\n",
       "  -0.023325296118855476,\n",
       "  -0.004914440680295229,\n",
       "  -0.02360435388982296,\n",
       "  -0.05296868085861206,\n",
       "  0.01385330781340599,\n",
       "  0.028758730739355087,\n",
       "  -0.04970237985253334,\n",
       "  -0.03407749533653259,\n",
       "  -0.00026929276646114886,\n",
       "  -0.013853064738214016,\n",
       "  0.004301130305975676,\n",
       "  -0.0012719406513497233,\n",
       "  -0.00878218561410904,\n",
       "  0.01594146341085434,\n",
       "  -0.0037024500779807568,\n",
       "  0.005592899862676859,\n",
       "  0.008028061129152775,\n",
       "  0.027481812983751297,\n",
       "  -0.008827512152493,\n",
       "  0.021424569189548492,\n",
       "  -0.052769679576158524,\n",
       "  -0.007043671328574419,\n",
       "  -0.011165576055645943,\n",
       "  -0.00246136961504817,\n",
       "  -0.006063556764274836,\n",
       "  -0.016082804650068283,\n",
       "  0.06792862713336945,\n",
       "  -0.017741641029715538,\n",
       "  -0.006370184011757374,\n",
       "  0.018856016919016838,\n",
       "  0.005752995610237122,\n",
       "  0.04444257915019989,\n",
       "  -0.025376608595252037,\n",
       "  -0.021073071286082268,\n",
       "  0.004856183659285307,\n",
       "  0.011831005103886127,\n",
       "  0.0335041880607605,\n",
       "  -0.052357565611600876,\n",
       "  0.03174016252160072,\n",
       "  0.036086127161979675,\n",
       "  0.04338376224040985,\n",
       "  -0.0133112957701087,\n",
       "  -0.026699189096689224,\n",
       "  -0.03809804469347,\n",
       "  -0.07397561520338058,\n",
       "  -0.05159471556544304,\n",
       "  0.05129016935825348,\n",
       "  0.06599286943674088,\n",
       "  0.010711484588682652,\n",
       "  0.009499795734882355,\n",
       "  -0.031171699985861778,\n",
       "  0.06990857422351837,\n",
       "  0.007257928140461445,\n",
       "  -0.008849483914673328,\n",
       "  -0.13357748091220856,\n",
       "  -0.03850459307432175,\n",
       "  0.028751613572239876,\n",
       "  0.024493927136063576,\n",
       "  0.02758277766406536,\n",
       "  0.009959030896425247,\n",
       "  -0.05145588144659996,\n",
       "  -0.059013720601797104,\n",
       "  -0.026756562292575836,\n",
       "  -0.08106929808855057,\n",
       "  0.03147318959236145,\n",
       "  -0.01989002153277397,\n",
       "  0.015758538618683815,\n",
       "  -0.044468820095062256,\n",
       "  0.06940365582704544,\n",
       "  -0.03254207223653793,\n",
       "  -0.012276696972548962,\n",
       "  0.09143689274787903,\n",
       "  0.005791198927909136,\n",
       "  -0.01065048947930336,\n",
       "  0.031461380422115326,\n",
       "  -0.01681443117558956,\n",
       "  -0.022814493626356125,\n",
       "  -0.002238111337646842,\n",
       "  -0.04958951100707054,\n",
       "  0.0051482669077813625,\n",
       "  0.011454030871391296,\n",
       "  -0.05081075429916382,\n",
       "  0.038243092596530914,\n",
       "  0.054081954061985016,\n",
       "  0.030122680589556694,\n",
       "  0.028520409017801285,\n",
       "  0.06746833771467209,\n",
       "  -0.041474033147096634,\n",
       "  0.03697003796696663,\n",
       "  -0.06182892620563507,\n",
       "  -0.03992069512605667,\n",
       "  -0.021449988707900047,\n",
       "  0.01618140935897827,\n",
       "  0.0069474889896810055,\n",
       "  -0.028289733454585075,\n",
       "  0.02151077799499035,\n",
       "  0.050706133246421814,\n",
       "  0.026001103222370148,\n",
       "  0.037769563496112823,\n",
       "  0.04360146075487137,\n",
       "  -0.004110563080757856,\n",
       "  0.06890490651130676,\n",
       "  0.010149485431611538,\n",
       "  0.013258714228868484,\n",
       "  0.023671282455325127,\n",
       "  -0.02374502830207348,\n",
       "  0.04933764040470123,\n",
       "  0.03390650823712349,\n",
       "  0.017314810305833817,\n",
       "  0.012408148497343063,\n",
       "  -0.04473915696144104,\n",
       "  -0.007650842424482107,\n",
       "  0.007412026170641184,\n",
       "  0.04265952855348587,\n",
       "  0.02555609494447708,\n",
       "  0.06266750395298004,\n",
       "  -0.024847200140357018,\n",
       "  0.037555526942014694,\n",
       "  0.013988824561238289,\n",
       "  -0.0382583811879158,\n",
       "  0.049359288066625595,\n",
       "  0.04330069199204445,\n",
       "  0.025438113138079643,\n",
       "  -0.012073557823896408,\n",
       "  0.05634957551956177,\n",
       "  -0.04886409267783165,\n",
       "  0.008559596724808216,\n",
       "  0.03988777846097946,\n",
       "  -0.043737489730119705,\n",
       "  -0.028506839647889137,\n",
       "  -0.004854239523410797,\n",
       "  -0.08999602496623993,\n",
       "  0.005221015773713589,\n",
       "  0.056032802909612656,\n",
       "  -0.011182906106114388,\n",
       "  -0.019600998610258102,\n",
       "  0.04349396750330925,\n",
       "  0.02342197299003601,\n",
       "  -0.004938820842653513,\n",
       "  0.08482804149389267,\n",
       "  0.0006875633262097836,\n",
       "  0.040567442774772644,\n",
       "  0.0033704801462590694,\n",
       "  0.009922701865434647,\n",
       "  -0.06560803204774857,\n",
       "  -0.0009402002324350178,\n",
       "  0.0037146001122891903,\n",
       "  0.007209910079836845,\n",
       "  0.03046451136469841,\n",
       "  0.005503132473677397,\n",
       "  0.05215781182050705,\n",
       "  -0.04386771842837334,\n",
       "  -0.01876516453921795,\n",
       "  0.03079693205654621,\n",
       "  -0.05207576975226402,\n",
       "  0.0027335251215845346,\n",
       "  0.03157482296228409,\n",
       "  -0.03610048443078995,\n",
       "  0.01099562831223011,\n",
       "  -0.03199157118797302,\n",
       "  -0.04322262853384018,\n",
       "  0.03739645332098007,\n",
       "  0.04831555858254433,\n",
       "  0.027204209938645363,\n",
       "  -0.028309619054198265,\n",
       "  0.06836842745542526,\n",
       "  0.006112124305218458,\n",
       "  -0.0592351108789444,\n",
       "  0.03191137686371803,\n",
       "  -0.0041740634478628635,\n",
       "  -0.023951247334480286,\n",
       "  -0.027181630954146385,\n",
       "  -0.015939537435770035,\n",
       "  -0.03869948163628578,\n",
       "  0.029947755858302116,\n",
       "  0.01984839141368866,\n",
       "  0.04251997172832489,\n",
       "  0.01961490325629711,\n",
       "  -0.0222160667181015,\n",
       "  0.0073857964016497135,\n",
       "  0.05100376531481743,\n",
       "  0.04574273154139519,\n",
       "  0.003527153516188264,\n",
       "  0.025516487658023834,\n",
       "  0.023633543401956558,\n",
       "  0.1068936213850975,\n",
       "  -0.02526167966425419,\n",
       "  -0.06493697315454483,\n",
       "  0.045971039682626724,\n",
       "  -0.06596777588129044,\n",
       "  0.00866529531776905,\n",
       "  -0.008595429360866547,\n",
       "  0.023561332374811172,\n",
       "  0.06034087762236595,\n",
       "  9.790082549443468e-05,\n",
       "  0.002565343864262104,\n",
       "  -0.013798767700791359,\n",
       "  0.023398125544190407,\n",
       "  -0.03487385809421539,\n",
       "  -0.018654631450772285,\n",
       "  -0.0033735886681824923,\n",
       "  -0.028598038479685783,\n",
       "  0.027300912886857986,\n",
       "  0.010596605949103832,\n",
       "  0.02777707204222679,\n",
       "  0.006246102042496204,\n",
       "  0.020107947289943695,\n",
       "  0.022334836423397064,\n",
       "  -0.05259978026151657,\n",
       "  0.015863236039876938,\n",
       "  0.09331853687763214,\n",
       "  0.03269876167178154,\n",
       "  -0.013479930348694324,\n",
       "  0.06290202587842941,\n",
       "  0.005040993448346853,\n",
       "  0.020900992676615715,\n",
       "  0.010220157913863659,\n",
       "  0.022078275680541992,\n",
       "  -0.0009012117516249418,\n",
       "  -0.05774423107504845,\n",
       "  0.036430202424526215,\n",
       "  0.0427962988615036,\n",
       "  0.06990672647953033,\n",
       "  -0.04000375047326088,\n",
       "  -0.00617254851385951,\n",
       "  -0.014527864754199982,\n",
       "  0.061169933527708054,\n",
       "  -0.014242704026401043,\n",
       "  0.005098266992717981,\n",
       "  -0.03093377687036991,\n",
       "  -0.05689194053411484,\n",
       "  -0.003365899436175823,\n",
       "  -0.013797455467283726,\n",
       "  -0.08211210370063782,\n",
       "  0.07317282259464264,\n",
       "  -0.03443816676735878,\n",
       "  -0.01935010589659214,\n",
       "  -0.025121428072452545,\n",
       "  0.03449583426117897,\n",
       "  0.02570226415991783,\n",
       "  -0.005388706456869841,\n",
       "  -0.029499920085072517,\n",
       "  0.021775154396891594,\n",
       "  -0.04946640133857727,\n",
       "  -0.025787455961108208,\n",
       "  -0.007210840005427599,\n",
       "  -0.03642210736870766,\n",
       "  -0.01934109441936016,\n",
       "  -0.01188831776380539,\n",
       "  0.012210797518491745,\n",
       "  -0.07026420533657074,\n",
       "  0.05092265084385872,\n",
       "  -0.01752580516040325,\n",
       "  0.0014355421299114823,\n",
       "  0.03555555269122124,\n",
       "  -0.028309930115938187,\n",
       "  0.037178948521614075,\n",
       "  -0.012841695919632912,\n",
       "  -0.04027155041694641,\n",
       "  0.001013062777929008,\n",
       "  0.009317723102867603,\n",
       "  0.05008364096283913,\n",
       "  -0.024635400623083115,\n",
       "  -0.0419272854924202,\n",
       "  -0.01908455230295658,\n",
       "  -0.021724384278059006,\n",
       "  -0.012566961348056793,\n",
       "  0.013578861951828003,\n",
       "  -0.07466281950473785,\n",
       "  -0.010322785004973412,\n",
       "  -0.03624969720840454,\n",
       "  0.0538722462952137,\n",
       "  -0.04463551938533783,\n",
       "  -0.04937514662742615,\n",
       "  0.0480361171066761,\n",
       "  0.008261225186288357,\n",
       "  0.015689656138420105,\n",
       "  0.021702665835618973,\n",
       "  -0.04993392899632454,\n",
       "  0.013353331945836544,\n",
       "  -0.0289485864341259,\n",
       "  -0.0021869176998734474,\n",
       "  -0.09528762847185135,\n",
       "  -0.011659863404929638,\n",
       "  0.00015679029456805438,\n",
       "  -0.010321443900465965,\n",
       "  -0.0481981486082077,\n",
       "  0.008378892205655575,\n",
       "  -0.014217732474207878,\n",
       "  0.02296898514032364,\n",
       "  -0.01693423092365265,\n",
       "  -0.05099160596728325,\n",
       "  -0.009046808816492558,\n",
       "  0.05942617729306221,\n",
       "  0.07297072559595108,\n",
       "  -0.021723685786128044,\n",
       "  0.028379028663039207,\n",
       "  -0.021280890330672264,\n",
       "  0.011808093637228012,\n",
       "  -0.015901945531368256,\n",
       "  0.04919765517115593,\n",
       "  0.013420246541500092,\n",
       "  0.001091938465833664,\n",
       "  0.015378783456981182,\n",
       "  0.010144215077161789,\n",
       "  -0.005698168650269508,\n",
       "  0.018106399103999138,\n",
       "  -0.016535766422748566,\n",
       "  0.004817331675440073,\n",
       "  -1.0638038475008216e-05,\n",
       "  -0.0068517946638166904,\n",
       "  -0.021566923707723618,\n",
       "  0.033420585095882416,\n",
       "  -0.022668415680527687,\n",
       "  -0.013042472302913666,\n",
       "  -0.052856069058179855,\n",
       "  0.046096835285425186,\n",
       "  -0.03158855438232422,\n",
       "  -0.04600232094526291,\n",
       "  0.012550483457744122,\n",
       "  0.013299955986440182,\n",
       "  0.003456184407696128,\n",
       "  -0.029805222526192665,\n",
       "  0.008713514544069767,\n",
       "  -0.019659386947751045,\n",
       "  -0.0483783520758152,\n",
       "  0.0005355508183129132,\n",
       "  0.0775839313864708,\n",
       "  0.006239942274987698,\n",
       "  0.02812984772026539,\n",
       "  0.05001910775899887,\n",
       "  -0.026405571028590202,\n",
       "  0.01586035266518593,\n",
       "  0.016048109158873558,\n",
       "  -0.037228427827358246,\n",
       "  0.0741669163107872,\n",
       "  -0.009979842230677605,\n",
       "  0.05715440213680267,\n",
       "  -0.002249920042231679,\n",
       "  0.01895160600543022,\n",
       "  0.04418382793664932,\n",
       "  0.005713135469704866,\n",
       "  -0.03578360751271248,\n",
       "  -0.008269940502941608,\n",
       "  0.007545687258243561,\n",
       "  -0.00708845816552639,\n",
       "  0.021519659087061882,\n",
       "  -0.008877238258719444,\n",
       "  0.07495294511318207,\n",
       "  0.0571664534509182,\n",
       "  -0.044123902916908264,\n",
       "  -0.004809787962585688,\n",
       "  -0.029711471870541573,\n",
       "  0.0409577339887619,\n",
       "  -0.021804019808769226,\n",
       "  -0.06337811797857285,\n",
       "  -0.03483700379729271,\n",
       "  0.015878427773714066,\n",
       "  -0.005420102272182703,\n",
       "  0.01777549274265766,\n",
       "  0.004741833079606295,\n",
       "  0.03329475224018097,\n",
       "  0.04205954074859619,\n",
       "  0.004416092298924923,\n",
       "  -0.016269531100988388,\n",
       "  0.06473833322525024,\n",
       "  0.015480944886803627,\n",
       "  -0.02337510511279106,\n",
       "  0.031178202480077744,\n",
       "  -0.005544349085539579,\n",
       "  0.023290511220693588,\n",
       "  0.07286971807479858,\n",
       "  0.015570943243801594,\n",
       "  0.010570424608886242,\n",
       "  -0.011629105545580387,\n",
       "  0.022913599386811256,\n",
       "  -0.041262149810791016,\n",
       "  -0.021207110956311226,\n",
       "  0.009078802540898323,\n",
       "  -0.026282265782356262,\n",
       "  -0.007205295376479626,\n",
       "  -0.058175500482320786,\n",
       "  -0.014472724869847298,\n",
       "  -0.03541521728038788,\n",
       "  0.005353172309696674,\n",
       "  -0.04925733059644699,\n",
       "  -0.0026120718102902174,\n",
       "  -0.04277215898036957,\n",
       "  -0.01452185120433569,\n",
       "  0.02512170933187008,\n",
       "  0.061412449926137924,\n",
       "  -0.006205388810485601,\n",
       "  -0.0417720228433609,\n",
       "  -0.08329909294843674,\n",
       "  -0.0424511656165123,\n",
       "  0.053132083266973495,\n",
       "  -0.026446407660841942,\n",
       "  -0.013922900892794132,\n",
       "  0.006307021714746952,\n",
       "  -0.020845435559749603,\n",
       "  -0.0183935035020113,\n",
       "  0.05246937274932861,\n",
       "  -0.010389852337539196,\n",
       "  -0.0696721151471138,\n",
       "  -0.046161167323589325,\n",
       "  0.027994301170110703,\n",
       "  0.005742884241044521,\n",
       "  0.014726132154464722,\n",
       "  -0.00509581109508872,\n",
       "  0.05032580718398094,\n",
       "  0.03159617632627487,\n",
       "  -0.017004627734422684,\n",
       "  0.011910217814147472,\n",
       "  -0.022507503628730774,\n",
       "  -0.04359043017029762,\n",
       "  5.264639185043052e-05,\n",
       "  0.03574662283062935,\n",
       "  -0.05459734797477722,\n",
       "  0.006011106073856354,\n",
       "  0.022188106551766396,\n",
       "  0.0008338703773915768,\n",
       "  -0.015022157691419125,\n",
       "  0.03489062935113907,\n",
       "  -0.035982899367809296,\n",
       "  -0.03482796996831894,\n",
       "  -0.037181511521339417,\n",
       "  0.0009934515692293644,\n",
       "  0.016559172421693802,\n",
       "  -0.09047669917345047,\n",
       "  0.025503944605588913,\n",
       "  -0.04923175647854805,\n",
       "  -0.0013555457117035985,\n",
       "  -0.03628673776984215,\n",
       "  -0.024283718317747116,\n",
       "  -0.04735202714800835,\n",
       "  -0.007391497027128935,\n",
       "  0.07758935540914536,\n",
       "  -0.002271423116326332,\n",
       "  -0.004237622953951359,\n",
       "  -0.0058636791072785854,\n",
       "  -0.03192273527383804,\n",
       "  -0.025377700105309486,\n",
       "  -0.07628101110458374,\n",
       "  0.018829049542546272,\n",
       "  -0.019021419808268547,\n",
       "  0.02874990925192833,\n",
       "  -0.013873768039047718,\n",
       "  -0.06131820008158684,\n",
       "  0.01919049769639969,\n",
       "  -0.011182935908436775,\n",
       "  -0.048727624118328094,\n",
       "  -0.0008304200600832701,\n",
       "  -0.05463416874408722,\n",
       "  -0.07482795417308807,\n",
       "  -0.010253689251840115,\n",
       "  -0.0740620344877243,\n",
       "  0.058877766132354736,\n",
       "  -0.019107989966869354,\n",
       "  -0.0225775558501482,\n",
       "  -0.005870885215699673,\n",
       "  0.01897623762488365,\n",
       "  0.0033016912639141083,\n",
       "  0.0033444794826209545,\n",
       "  -0.03412802517414093,\n",
       "  -0.0013306672917678952,\n",
       "  0.006667278707027435,\n",
       "  0.0003591006970964372,\n",
       "  -0.05450191721320152,\n",
       "  0.05472099781036377,\n",
       "  0.022969171404838562,\n",
       "  0.006784701254218817,\n",
       "  0.005020027048885822,\n",
       "  -0.05416659638285637,\n",
       "  0.00753046665340662,\n",
       "  0.014620673842728138,\n",
       "  -0.003085972974076867,\n",
       "  0.031529031693935394,\n",
       "  0.05993708223104477,\n",
       "  0.0012999308528378606,\n",
       "  -0.03668896481394768,\n",
       "  -0.015724681317806244,\n",
       "  -0.036716874688863754,\n",
       "  -0.015167937614023685,\n",
       "  0.07645326107740402,\n",
       "  -0.061043236404657364,\n",
       "  0.02927161380648613,\n",
       "  0.053545910865068436,\n",
       "  -0.020098542794585228,\n",
       "  -0.027084844186902046,\n",
       "  0.016946181654930115,\n",
       "  0.0275795366615057,\n",
       "  0.021570686250925064,\n",
       "  0.01592315174639225,\n",
       "  0.06271208077669144,\n",
       "  -0.047355495393276215,\n",
       "  -0.030040990561246872,\n",
       "  0.013808212243020535,\n",
       "  -0.022229744121432304,\n",
       "  0.014874947257339954,\n",
       "  0.023703640326857567,\n",
       "  -0.05499061942100525,\n",
       "  -0.10002490133047104,\n",
       "  -0.00851358287036419,\n",
       "  0.01592022366821766,\n",
       "  -0.044101279228925705,\n",
       "  -0.004676709417253733,\n",
       "  0.0017634237883612514,\n",
       "  -0.01487178634852171,\n",
       "  0.04859156161546707,\n",
       "  -0.020174387842416763,\n",
       "  0.058309126645326614,\n",
       "  -0.055682942271232605,\n",
       "  -0.009138180874288082,\n",
       "  0.01316694263368845,\n",
       "  -0.02727348729968071,\n",
       "  0.02303793840110302,\n",
       "  0.0033206797670572996,\n",
       "  -0.004757624119520187,\n",
       "  -0.019484976306557655,\n",
       "  0.040214404463768005,\n",
       "  0.01868406869471073,\n",
       "  0.035079121589660645,\n",
       "  0.006939563900232315,\n",
       "  0.002948179142549634,\n",
       "  -0.01731942966580391,\n",
       "  0.004575295839458704,\n",
       "  -0.08545096218585968,\n",
       "  0.04002844914793968,\n",
       "  -0.047580212354660034,\n",
       "  -0.025190412998199463,\n",
       "  0.07398615032434464,\n",
       "  0.042286474257707596,\n",
       "  -0.03143014758825302,\n",
       "  0.02603919617831707,\n",
       "  0.019845599308609962,\n",
       "  -0.0023793566506356,\n",
       "  0.0421801432967186,\n",
       "  0.025236638262867928,\n",
       "  -0.018697625026106834,\n",
       "  -0.010293190367519855,\n",
       "  0.031086590141057968,\n",
       "  0.038006410002708435,\n",
       "  0.006277188658714294,\n",
       "  0.044032156467437744,\n",
       "  0.009295675903558731,\n",
       "  -0.0020907914731651545,\n",
       "  -0.009647387079894543,\n",
       "  0.033273983746767044,\n",
       "  -0.025321131572127342,\n",
       "  -0.013435565866529942,\n",
       "  -0.015835417434573174,\n",
       "  -0.02111818455159664,\n",
       "  -0.009291887283325195,\n",
       "  -0.003913434688001871,\n",
       "  -0.03713518753647804,\n",
       "  -0.06084756553173065,\n",
       "  0.013007855974137783,\n",
       "  -0.04435382038354874,\n",
       "  0.030181029811501503,\n",
       "  0.07106303423643112,\n",
       "  -0.008929906412959099,\n",
       "  -0.06261375546455383,\n",
       "  0.05940013751387596,\n",
       "  0.006091734394431114,\n",
       "  -0.014880378730595112,\n",
       "  0.07906969636678696,\n",
       "  0.05397041514515877,\n",
       "  0.013072906993329525,\n",
       "  -0.013040181249380112,\n",
       "  -0.013443704694509506,\n",
       "  0.06931852549314499,\n",
       "  -0.010462988168001175,\n",
       "  0.03304227069020271,\n",
       "  -0.025425763800740242,\n",
       "  0.01821204461157322,\n",
       "  0.030858686193823814,\n",
       "  -0.027870191261172295,\n",
       "  -0.017150549218058586,\n",
       "  0.003924879245460033,\n",
       "  -0.014913122169673443,\n",
       "  -0.021991394460201263,\n",
       "  0.07636606693267822,\n",
       "  -0.04850447177886963,\n",
       "  0.02494594268500805,\n",
       "  0.01624993234872818,\n",
       "  0.007360127288848162,\n",
       "  0.021785303950309753,\n",
       "  0.01934000849723816,\n",
       "  0.06640453636646271,\n",
       "  0.0023586067836731672,\n",
       "  -0.027187705039978027,\n",
       "  0.002210621489211917,\n",
       "  -0.04248948395252228,\n",
       "  -0.0324600525200367,\n",
       "  0.011355549097061157,\n",
       "  0.04180802404880524,\n",
       "  0.03817173093557358,\n",
       "  -0.010706335306167603,\n",
       "  -0.030538419261574745,\n",
       "  -0.01250410359352827,\n",
       "  0.01977570727467537,\n",
       "  0.022063547745347023,\n",
       "  -0.0039263153448700905,\n",
       "  0.06016169860959053,\n",
       "  0.026587437838315964,\n",
       "  -0.011306078173220158,\n",
       "  -0.03306588530540466,\n",
       "  0.0829537883400917,\n",
       "  0.04232856258749962,\n",
       "  0.05370744690299034,\n",
       "  0.0381082259118557,\n",
       "  -0.012372918426990509,\n",
       "  -0.02724592387676239,\n",
       "  -0.05735567584633827,\n",
       "  -0.010029789991676807,\n",
       "  -0.022982945665717125,\n",
       "  0.02899818681180477,\n",
       "  0.01378904189914465,\n",
       "  -0.0090226661413908,\n",
       "  -0.0898558646440506,\n",
       "  -0.026820708066225052,\n",
       "  -0.021107500419020653,\n",
       "  -0.0377141498029232,\n",
       "  -0.035156626254320145,\n",
       "  0.024148091673851013,\n",
       "  0.025266531854867935,\n",
       "  -0.11454220116138458,\n",
       "  -0.04940009117126465,\n",
       "  0.00013241767010185868,\n",
       "  -0.013546344824135303,\n",
       "  0.004352150019258261,\n",
       "  0.01300481054931879,\n",
       "  0.0016258398536592722,\n",
       "  -0.008608474396169186,\n",
       "  0.0339268334209919,\n",
       "  -0.011779597029089928,\n",
       "  -0.019071010872721672,\n",
       "  0.07273130118846893,\n",
       "  -0.03768635541200638,\n",
       "  -0.03866045922040939,\n",
       "  -0.011319877579808235,\n",
       "  -0.00588688300922513,\n",
       "  -0.01536430511623621,\n",
       "  0.02189650386571884,\n",
       "  -0.05968549847602844,\n",
       "  -0.03804367408156395,\n",
       "  -0.07610968500375748,\n",
       "  -0.03649606183171272,\n",
       "  -0.030078405514359474,\n",
       "  -0.06147342547774315,\n",
       "  0.021973231807351112,\n",
       "  -0.001115969498641789,\n",
       "  -0.021999841555953026,\n",
       "  0.03430226817727089,\n",
       "  0.010766166262328625,\n",
       "  0.006127584259957075,\n",
       "  0.03355833515524864,\n",
       "  0.009074022062122822,\n",
       "  -0.009820548817515373,\n",
       "  0.02522287704050541,\n",
       "  -0.009488283656537533,\n",
       "  -0.010184063576161861,\n",
       "  0.014026688411831856,\n",
       "  -0.0125260716304183,\n",
       "  0.009654514491558075,\n",
       "  0.03271017596125603,\n",
       "  -0.0029123544227331877,\n",
       "  -0.03183171525597572,\n",
       "  -0.049692489206790924,\n",
       "  -0.0330347865819931,\n",
       "  -0.06317395716905594,\n",
       "  0.014501591213047504,\n",
       "  0.03086196631193161,\n",
       "  0.02831682562828064,\n",
       "  0.0019454662688076496,\n",
       "  -0.020237449556589127,\n",
       "  -0.0053175706416368484,\n",
       "  0.02510293386876583,\n",
       "  0.051493242383003235,\n",
       "  -0.025227557867765427,\n",
       "  -0.04521015286445618,\n",
       "  0.013269523158669472,\n",
       "  0.035957079380750656,\n",
       "  -0.005014442373067141,\n",
       "  0.01332826353609562,\n",
       "  0.06229805946350098,\n",
       "  -0.027990706264972687,\n",
       "  0.04019508883357048,\n",
       "  0.04820703715085983,\n",
       "  0.051428861916065216,\n",
       "  -0.06632622331380844,\n",
       "  -0.017453718930482864,\n",
       "  0.02922026254236698,\n",
       "  -0.0027761345263570547,\n",
       "  -0.033929578959941864,\n",
       "  -0.012844854965806007,\n",
       "  0.05690465867519379,\n",
       "  -0.01824287511408329,\n",
       "  0.07907113432884216,\n",
       "  0.0541485957801342,\n",
       "  0.05295782908797264,\n",
       "  0.009323231875896454,\n",
       "  -0.040853213518857956,\n",
       "  -0.07720492035150528,\n",
       "  0.06263213604688644,\n",
       "  -0.011392302811145782,\n",
       "  -0.04411783069372177,\n",
       "  -0.04529651626944542,\n",
       "  -0.003294246504083276,\n",
       "  0.05484478175640106,\n",
       "  -0.020599283277988434,\n",
       "  -0.013365094549953938,\n",
       "  0.008178512565791607,\n",
       "  -0.014216949231922626,\n",
       "  0.06073598936200142,\n",
       "  0.019875355064868927,\n",
       "  0.03868130221962929,\n",
       "  -0.027213355526328087,\n",
       "  -0.0758790522813797,\n",
       "  -0.07141417264938354,\n",
       "  0.028145616874098778,\n",
       "  0.025025280192494392,\n",
       "  0.07092800736427307,\n",
       "  0.0345354788005352,\n",
       "  -0.03192824125289917,\n",
       "  0.008146663196384907,\n",
       "  -0.03127767890691757,\n",
       "  0.005334630608558655,\n",
       "  -0.0467037670314312,\n",
       "  -0.041477661579847336,\n",
       "  -0.009941638447344303,\n",
       "  0.00847071036696434,\n",
       "  0.0305938757956028,\n",
       "  0.051349010318517685,\n",
       "  -0.02436627633869648,\n",
       "  -0.012040489353239536,\n",
       "  -0.011746537871658802,\n",
       "  -0.004434431903064251,\n",
       "  0.01625373587012291,\n",
       "  -0.03822743520140648,\n",
       "  0.0010154636111110449,\n",
       "  -0.01397075317800045,\n",
       "  -0.0222074706107378,\n",
       "  0.03669200837612152,\n",
       "  0.02118060737848282,\n",
       "  -0.013652064837515354,\n",
       "  0.036495089530944824]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model.embed_documents([docs[0].page_content])  # Embedding the first chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26a25394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_model.embed_documents([docs[0].page_content])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d767132b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "98fd5d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(\n",
    "    docs,\n",
    "    embedding_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a9129901",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_doc = vectorstore.similarity_search(\"What is the llama2 llm model?\")  # Searching for similar chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28ab4018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'See evaluations for pretraining (Section 2); fine-tuning (Section 3); and safety (Section 4).\\nEthical Considerations and Limitations(Section 5.2)\\nLlama 2is a new technology that carries risks with use. Testing conducted to date has been in\\nEnglish, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs,\\nLlama 2’s potential outputs cannot be predicted in advance, and the model may in some instances'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_doc[0].page_content  # Accessing the content of the first relevant chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1a7cd482",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_doc1 = vectorstore.similarity_search(\"llama2 finetuning benchmark experiments\", k=3)  # Searching for top 3 similar chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2fb03368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ba. Large language models are human-level prompt engineers. InThe Eleventh International Conference on\\nLearning Representations, 2022.\\n44'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_doc1[2].page_content  # Accessing the content of the second relevant chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d44c43c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()  # Creating a retriever from the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ecb06c22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='2c4fd8d5-72c9-4618-bc08-1785d270e76d', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'd:\\\\llmops\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 7, 'page_label': '8'}, page_content='13B 18.9 66.1 52.6 62.3 10.9 46.9 37.0 33.9\\n33B 26.0 70.0 58.4 67.6 21.4 57.8 39.8 41.7\\n65B 30.7 70.7 60.5 68.6 30.8 63.4 43.5 47.6\\nLlama 2\\n7B 16.8 63.9 48.9 61.3 14.6 45.3 32.6 29.3\\n13B 24.5 66.9 55.4 65.8 28.7 54.8 39.4 39.1\\n34B 27.8 69.9 58.7 68.0 24.2 62.6 44.1 43.4\\n70B 37.5 71.9 63.6 69.4 35.2 68.9 51.2 54.2\\nTable 3: Overall performance on grouped academic benchmarks compared to open-source base models.'),\n",
       " Document(id='ab784471-b95b-43ac-9b95-fbfb8c04e545', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'd:\\\\llmops\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 73, 'page_label': '74'}, page_content='Llama 2\\n7B 0.28 0.25 0.29 0.50 0.36 0.37 0.21 0.34 0.32 0.50 0.28 0.19 0.26 0.32 0.44 0.51 0.30 0.2513B 0.24 0.25 0.35 0.50 0.41 0.36 0.24 0.39 0.35 0.48 0.31 0.18 0.27 0.34 0.46 0.66 0.35 0.2834B 0.27 0.24 0.33 0.56 0.41 0.36 0.26 0.32 0.36 0.53 0.33 0.07 0.26 0.30 0.45 0.56 0.26 0.3570B 0.31 0.29 0.35 0.51 0.41 0.45 0.27 0.34 0.40 0.52 0.36 0.12 0.28 0.31 0.45 0.65 0.33 0.20\\nFine-tuned'),\n",
       " Document(id='6f2cb44c-cc09-4b47-869e-866c18c2ac15', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'd:\\\\llmops\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 70, 'page_label': '71'}, page_content='65B 14.27 31.59 21.90 14.89 23.51 22.27 17.16 18.91 28.40 19.32 28.71 22.00 20.03\\nLlama 2\\n7B 16.53 31.15 22.63 15.74 26.87 19.95 15.79 19.55 25.03 18.92 21.53 22.34 20.20\\n13B 21.29 37.25 22.81 17.77 32.65 24.13 21.05 20.19 35.40 27.69 26.99 28.26 23.84\\n34B 16.76 29.63 23.36 14.38 27.43 19.49 18.54 17.31 26.38 18.73 22.78 21.66 19.04\\n70B 21.29 32.90 25.91 16.92 30.60 21.35 16.93 21.47 30.42 20.12 31.05 28.43 22.35\\nFine-tuned\\nChatGPT 0.23 0.22 0.18 0 0.19 0 0.46 0 0.13 0 0.47 0 0.66'),\n",
       " Document(id='cfe264be-ef0e-4d50-ba1d-1f24ed4d58e2', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'd:\\\\llmops\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 6, 'page_label': '7'}, page_content='models internally. For these models, we always pick the best score between our evaluation framework and\\nany publicly reported results.\\nIn Table 3, we summarize the overall performance across a suite of popular benchmarks. Note that safety\\nbenchmarks are shared in Section 4.1. The benchmarks are grouped into the categories listed below. The\\nresults for all the individual benchmarks are available in Section A.2.2.')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"llama2 finetuning benchmark experiments.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "00d60f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "        Answer the question based on the context provided below. \n",
    "        If the context does not contain sufficient information, respond with: \n",
    "        \"I do not have enough information about this.\"\n",
    "\n",
    "        Context: {context}\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Answer:\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "462fba6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8e3f4c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a5d8f50b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='\\n        Answer the question based on the context provided below. \\n        If the context does not contain sufficient information, respond with: \\n        \"I do not have enough information about this.\"\\n\\n        Context: {context}\\n\\n        Question: {question}\\n\\n        Answer:')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "83c32097",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0712e84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser=StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6fd3c9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7f44b07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c393462b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatGroq(model=\"deepseek-r1-distill-llama-70b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "65dc9806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\n\\n</think>\\n\\nThe capital of France is Paris.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is the capital of France?\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "25968494",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0523d24a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<think>\\nOkay, I need to answer the question about the Llama 2 fine-tuning benchmark experiments based on the provided context. Let me look through the context carefully.\\n\\nFirst, I see two tables mentioned: Table 3 and another table with fine-tuned models. In Table 3, there are rows for Llama 2 with different sizes (7B, 13B, 34B, 70B) and various benchmark scores. The context also mentions that for these models, the best score between their evaluation framework and public results is chosen. This suggests that the performance is evaluated across multiple benchmarks, grouped into categories.\\n\\nLooking at the fine-tuned section, there's another table listing Llama 2 models with different sizes and a series of numbers. These numbers might represent performance metrics across different tasks or datasets. The context also mentions that safety benchmarks are discussed in Section 4.1, but that's not provided here.\\n\\nI should explain that the experiments involve evaluating Llama 2 models of various sizes against grouped academic benchmarks. The results are summarized in Table 3, which compares their performance. The fine-tuning process likely involves optimizing these models for specific tasks, and the scores show how each model size performs across different categories.\\n\\nI should also note that the exact details of the experiments, such as the specific tasks or the evaluation framework, aren't fully detailed in the provided context. Therefore, I might have to mention that while the context shows the results, it doesn't provide all the experimental details.\\n\\nSo, putting it together, I'll explain that the experiments compare different Llama 2 models across benchmarks, showing their performance improvements after fine-tuning, but without more context on the specifics of the experiments themselves.\\n</think>\\n\\nThe context provided includes information about Llama 2 models and their performance on various benchmarks, both in their base forms and after fine-tuning. Here's a summary of the details related to the fine-tuning benchmark experiments:\\n\\n1. **Model Sizes and Performance**: The Llama 2 models come in different sizes, including 7B, 13B, 34B, and 70B. Each model's performance is evaluated on a suite of academic benchmarks, with the results summarized in Table 3. The table shows the overall performance across grouped categories, indicating how each model size compares against others.\\n\\n2. **Fine-Tuning Results**: The fine-tuned versions of Llama 2 models are listed with specific performance metrics. These metrics likely represent scores across various tasks or datasets, showing the improvement or specialization achieved through fine-tuning.\\n\\n3. **Evaluation Framework**: The context mentions that for these models, the best score between their internal evaluation framework and any publicly reported results is chosen. This suggests a comprehensive approach to assessing model performance, ensuring that the results are as robust as possible.\\n\\n4. **Benchmark Grouping**: The benchmarks are grouped into categories, though the exact categories are not detailed in the provided context. The individual benchmark results are referenced in Section A.2.2, which is not included here.\\n\\n5. **Safety Benchmarks**: While the context mentions that safety benchmarks are shared in Section 4.1, this section is not provided, so details on safety-specific evaluations are unavailable.\\n\\nIn summary, the fine-tuning benchmark experiments for Llama 2 involve evaluating different model sizes across various academic benchmarks, with results showing their performance. The process includes selecting the best scores from internal and public evaluations. However, specific details about the evaluation framework and exact tasks are not provided in the context.\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"tell  me about the llama2 finetuning benchmark experiments?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ca3289ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<think>\\nAlright, I need to figure out the scaling trends for the reward model based on the provided context. Let me read through the context carefully.\\n\\nFirst, the context mentions Figure 6, which reports the scaling trends. It states that larger models achieve higher performance with a similar volume of data. So, as the model size increases, the performance improves, which is a positive sign.\\n\\nNext, it says that the scaling performance hasn't plateaued yet. That means even with the current amount of training data, the models can still improve if more data is added. This indicates there's room for further enhancements with additional annotations.\\n\\nAlso, the context points out that the reward model accuracy is one of the most important factors. This suggests that the model's performance is significantly influenced by its accuracy, which is tied to the scaling trends.\\n\\nLooking further, there's a mention of evaluating the impact of margin-based loss on reward score distribution shifts. Figure 27 shows a histogram of reward scores, and the margin term pushes the reward scores higher. This might mean that the distribution of scores becomes more spread out, potentially improving the model's ability to distinguish between better and worse responses.\\n\\nFigure 29 relates the average reward model score to the model response quality ratings from human reviews. It uses a 7-point Likert scale for helpfulness and safety. The shaded areas show the standard deviation, which gives an idea of the consistency of the scores. This indicates that as the model's average reward score increases, the quality ratings from humans also improve, showing a positive correlation.\\n\\nPutting this together, the scaling trends for the reward model are positive. Larger models perform better with the same data, and there's potential for more improvement as more data is annotated. The use of margin-based loss helps in managing the reward score distribution, and the model's scores correlate well with human evaluations of quality.\\n</think>\\n\\nThe scaling trends for the reward model indicate that larger models achieve higher performance with similar data volumes. Additionally, the scaling performance has not yet plateaued, suggesting potential for further improvement with more data annotations. The reward model accuracy is a key factor, and the margin-based loss positively influences reward score distributions, which correlate well with human quality ratings.\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"can you tell me Scaling trends for the reward model?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d524877",
   "metadata": {},
   "source": [
    "# One Small task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99578206",
   "metadata": {},
   "source": [
    "### Take 10 pdfs keep it in same directory and create RAG on top of it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de5770f",
   "metadata": {},
   "source": [
    "In next class will discuss about the(will start with the modular coding)\n",
    "1. exception module\n",
    "2. logger module\n",
    "3. doc analyser\n",
    "4. doc compare\n",
    "5. utils and config\n",
    "   \n",
    "2 class\n",
    "\n",
    "2 more class api and other module\n",
    "\n",
    "2 more class for deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4472280c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
